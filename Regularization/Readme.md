Regularization
===

Weights
---
* L2 regularization (weight decay): 
  * term d(x _dot x)/dx = 2x expotentially decays the weights.
* L1 regularization: model sparsity 
  * when two features correlate well with the target, one features' weights will be approximately 0 (in linear models)

Data
---
* Dataset augmentation: image rotation, cropping etc.
* Adversarial examples

Multi-task learning
---
* Generalize the base model to perform well on multiple models

Early stopping
---

Bagging and Ensemble
---
* Dropout is a kind of ensembling

