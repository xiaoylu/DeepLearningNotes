Information-theoretical Measures
===

Kullbackâ€“Leibler divergence
---
KL divergence is a measure of how one probability distribution is different from a second, reference probability distribution.
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a32176917e2304cf7c3a1e59220bf303d7f136c6)

Mutual Information
---
Definition:
![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8da24e3338c5cadd04dd823feb3fbd85d95c611)

MI is the amount of information gained from observing another variable
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/209285ec1c887eaef3321b960b115857d8b1c099)

Conditional Entropy
---
CE is the amount of uncertainty remaining about Y after X is known.
