Regularization
===

Weights
---
* L2 regularization (weight decay): term d(x _dot x)/dx = 2x expotentially decays the weights.
* L1 regularization: model sparsity 

Data
---
* Dataset augmentation: image rotation, cropping etc.
* Adversarial examples

Multi-task learning
---
* Generalize the base model to perform well on multiple models

Early stopping
---

Bagging and Ensemble
---
* Dropout is a kind of ensembling

