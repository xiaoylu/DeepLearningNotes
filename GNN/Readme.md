Graph Neural Network
===

GNN takes relational data as input and outputs the node embeddings

Slides for a general review: http://tkipf.github.io/misc/SlidesCambridge.pdf

Shallow Models
===
DeepWalk
---
As in Skip-Gram, each vertex v in DeepWalk also has two representation vectors :

`v` is center word and `c` is the context word

DeepWalk maximizes the average log probability of predicting context vertices `P(c|v) = sigmoid(c^T *v)`
along the walk sequence

Node2Vec
---
Node2Vec argues that the walk sequence construction is not good enough in DeepWalk.

It applies a second order random walk strategy to sample the neighborhood nodes, 
interpolate between breadth-first sampling (BFS) and depth-first sampling (DFS).

In this way, the nodes sharing **similar roles** in a network also have similar embeddings

LINE
---
LINE explicitly construct the second order proximity. It minimizes the KL-divergence of the distributions generated by 
* first order proximity as `sigmoid(c*v)`
* second order proximity as `softmax(c, v)` given a set of context nodes `{c1,c2,..,c_k}`.
and the empirical distributions.

Deep Models
===
SDNE (autoencoder)
---
Deep autoencoder encode node vector `x` as `y` and mininize the Laplacian eigenmaps between `y_i` and `y_j` if `(i,j) in E`.
Also, the second order proximity is perserved as the decoded `x'` should be similar to the node vector `x`.

Graph Conv (Kipf & Welling)
---
https://tkipf.github.io/graph-convolutional-networks/

For each layer, `H^l = ReLU(L H^(l-1))` where `L` is the normalized graph laplacian `D^-1/2 A D^-1/2`. The input is a set of node feature vectors

Applications
===

Recommendations
---

