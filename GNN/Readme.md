Graph Neural Network
===

GNN takes relational data as input and outputs the node embeddings

Shallow Models
===
DeepWalk
---
As in Skip-Gram, each vertex v in DeepWalk also has two representation vectors :

`v` is center word and `c` is the context word

DeepWalk maximizes the average log probability of predicting context vertices `P(c|v) = sigmoid(c^T *v)`
along the walk sequence

Node2Vec
---
Node2Vec argues that the walk sequence construction is not good enough in DeepWalk.

It applies a second order random walk strategy to sample the neighborhood nodes, 
interpolate between breadth-first sampling (BFS) and depth-first sampling (DFS).

In this way, the nodes sharing **similar roles** in a network also have similar embeddings

LINE
---
LINE explicitly construct the second order proximity. It minimizes the KL-divergence of the distributions generated by 
* first order proximity as `sigmoid(c*v)`
* second order proximity as `softmax(c, v)` given a set of context nodes `{c1,c2,..,c_k}`.
and the empirical distributions.

Deep Models
===
Graph Conv (Kipf & Welling)
---
