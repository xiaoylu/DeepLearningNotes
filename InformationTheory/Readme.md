Information-theoretical Measures
===

Kullbackâ€“Leibler divergence
---
KL divergence is a measure of how one probability distribution is different from a second, reference probability distribution.

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a32176917e2304cf7c3a1e59220bf303d7f136c6)

Mutual Information
---
Definition:

![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8da24e3338c5cadd04dd823feb3fbd85d95c611)

MI is the amount of information gained from observing another variable

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/209285ec1c887eaef3321b960b115857d8b1c099)

which is the entropy of `Y` minus the conditional entropy of `Y` given `X`.

Conditional Entropy
---
CE is the amount of uncertainty remaining about Y after X is known.

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c200b367c0f09c8d1faad3319c6c393d3ebbe539)
